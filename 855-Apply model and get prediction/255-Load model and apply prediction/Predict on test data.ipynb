{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path():\n",
    "    return '/Users/thibaultdesfontaines/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(get_data_path(), \"leaf-classification\")\n",
    "train_dir = os.path.join(data_dir, 'train_images')\n",
    "validation_dir = os.path.join(data_dir, 'validation_images')\n",
    "test_dir = os.path.join(data_dir, 'test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 99\n"
     ]
    }
   ],
   "source": [
    "CLASS_NAMES = [x for x in sorted(os.listdir(str(train_dir))) if x[0] != '.']\n",
    "CLASS_NAMES = np.array(CLASS_NAMES)\n",
    "print(\"Number of classes: {}\".format(len(CLASS_NAMES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thibaultdesfontaines/data/training_1/cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"/Users/thibaultdesfontaines/data/training_1/\"\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "print(\"{}\".format(latest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.2),\n",
    "        Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(99)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               25690624  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 99)                50787     \n",
      "=================================================================\n",
      "Total params: 25,764,995\n",
      "Trainable params: 25,764,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1113e6cd0> and <tensorflow.python.keras.layers.core.Dropout object at 0x12a99e3d0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1113e6a50> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x1113e6ad0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x1113e8310> and <tensorflow.python.keras.layers.core.Dropout object at 0x106c83910>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x1113e83d0> and <tensorflow.python.keras.layers.core.Flatten object at 0x1113e8110>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x139a492d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = {}\n",
    "for image_path in os.listdir(test_dir):\n",
    "    to_predict[image_path] = {}\n",
    "    to_predict[image_path]['done'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(os.path.join(test_dir, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thibaultdesfontaines/data/leaf-classification/test_images/567.jpg\n",
      "/Users/thibaultdesfontaines/data/leaf-classification/test_images/686.jpg\n",
      "/Users/thibaultdesfontaines/data/leaf-classification/test_images/534.jpg\n",
      "/Users/thibaultdesfontaines/data/leaf-classification/test_images/594.jpg\n",
      "/Users/thibaultdesfontaines/data/leaf-classification/test_images/474.jpg\n"
     ]
    }
   ],
   "source": [
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    name = get_name(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare_for_training(labeled_ds)\n",
    "\n",
    "image_batch, label_batch = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist(predictions, edgecolor='k', alpha=0.8, bins=99)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_batch = np.array(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "for pred, label in zip(predictions, label_batch):\n",
    "    pred_dict[label] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = np.zeros((len(pred_dict), 100), dtype=int)\n",
    "for i, label in enumerate(pred_dict):\n",
    "    index_value = int(label.split('.')[0])\n",
    "    prediction_matrix[i][0] = index_value\n",
    "    prediction_value = pred_dict[label]\n",
    "    prediction_matrix[i][prediction_value+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 640,    0,    0, ...,    0,    0,    0],\n",
       "       [ 205,    0,    0, ...,    0,    0,    0],\n",
       "       [1012,    0,    0, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1455,    0,    0, ...,    0,    0,    0],\n",
       "       [ 221,    0,    0, ...,    0,    0,    0],\n",
       "       [1214,    0,    0, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prediction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id']+list(CLASS_NAMES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
